library(DT)
library(tidytext)
library(dplyr)
library(stringr)
library(sentimentr)
library(ggplot2)
library(RColorBrewer)
library(readr)
library(SnowballC)
library(tm)
library(wordcloud)
library(reticulate)
library(crfsuite)
customer_reviews <- read.delim(file.choose())
str(customer_reviews)
attach(customer_reviews)
str(customer_reviews)
customer_reviews <- as.character(customer_reviews)
customer_courpus <- Corpus(VectorSource(comments))
print(customer_courpus)
customer_courpus <- tm_map(customer_courpus, tolower)
inspect(customer_courpus[1:6])
customer_courpus <- tm_map(customer_courpus,removePunctuation)
inspect(customer_courpus[1:6])
customer_courpus <- tm_map(customer_courpus,removeNumbers)
inspect(customer_courpus[1:6])
customer_courpus <-tm_map(customer_courpus,stripWhitespace)
inspect(customer_courpus[1:6])
cleanset <- tm_map(customer_courpus,removeWords, stopwords('english'))
inspect(cleanset[1:6])
tdm <- TermDocumentMatrix(cleanset)
tdm
tdm <- as.matrix(tdm)
tdm[1:10,1:20]
w <- rowSums(tdm)  # provides the no of times a particular word has been used.
w <- subset(w, w>=10) # Pull words that were used more than 10 times.
barplot(w, las = 2, col = rainbow(50))
# Word Cloud :
w <- sort(rowSums(tdm), decreasing = TRUE) # Sort words in decreasing order.
set.seed(123)
wordcloud(words = names(w), freq = w,
max.words = 250,random.order = F,
min.freq =  3,
colors = brewer.pal(8, 'Dark2'),
scale = c(5,0.3),
rot.per = 0.6)
# Word Cloud :
w <- sort(rowSums(tdm), decreasing = TRUE) # Sort words in decreasing order.
set.seed(123)
wordcloud(words = names(w), freq = w,
max.words = 250,random.order = F,
min.freq =  3,
colors = brewer.pal(8, 'Dark2'),
scale = c(5,0.3),
rot.per = 0.6)
library(wordcloud2)
w <- data.frame(names(w),w)
colnames(w) <- c('word','freq')
wordcloud2(w,size = 0.5, shape = 'triangle', rotateRatio = 0.5, minSize = 1)
#Sentiment Analysis
library(syuzhet)
library(lubridate)
library(scales)
library(reshape2)
customer_rvws <- read.delim(file.choose())
customer_reviews <- iconv(customer_rvws$comments)
s <- get_nrc_sentiment(customer_reviews)
get_nrc_sentiment('delay')
get_nrc_sentiment('ugly')
s
consumer_reviews <- readLines(file.choose())
consumer = c(customer_reviews)
result=score.sentiment(consumer,pos.words, neg.words)
result
result$score
consumer_reviews <- readLines(file.choose())
consumer = c(customer_reviews)
result=score.sentiment(consumer,pos.words, neg.words)
pos.words = scan(file.choose(), what='character', comment.char=';')
neg.words = scan(file.choose(), what='character', comment.char=';')
neg.words = c(neg.words, 'wtf', 'fail')
#Implementing our sentiment scoring algorithm
require(plyr)
require(stringr)
require(stringi)
score.sentiment = function(sentences, pos.words, neg.words, .progress='none')
{
# we got a vector of sentences. plyr will handle a list
# or a vector as an "l" for us
# we want a simple array of scores back, so we use
# "l" + "a" + "ply" = "laply":
scores = laply(sentences, function(sentence, pos.words, neg.words) {
# clean up sentences with R's regex-driven global substitute, gsub():
sentence = gsub('[[:punct:]]', '', sentence)
sentence = gsub('[[:cntrl:]]', '', sentence)
sentence = gsub('\\d+', '', sentence)
# and convert to lower case:
sentence = tolower(sentence)
# split into words. str_split is in the stringr package
word.list = str_split(sentence, '\\s+')
# sometimes a list() is one level of hierarchy too much
words = unlist(word.list)
# compare our words to the dictionaries of positive & negative terms
pos.matches = match(words, pos.words)
neg.matches = match(words, neg.words)
# match() returns the position of the matched term or NA
# we just want a TRUE/FALSE:
pos.matches = !is.na(pos.matches)
neg.matches = !is.na(neg.matches)
# and conveniently enough, TRUE/FALSE will be treated as 1/0 by sum():
score = sum(pos.matches) - sum(neg.matches)
return(score)
}, pos.words, neg.words, .progress=.progress )
scores.df = data.frame(score=scores, text=sentences)
return(scores.df)
}
sentiment.scores= score.sentiment(customer_reviews, pos.words, neg.words, .progress='none')
score <- sentiment.scores$score
score
#Implementing our sentiment scoring algorithm
require(plyr)
require(stringr)
require(stringi)
score.sentiment = function(sentences, pos.words, neg.words, .progress='none')
{
# we got a vector of sentences. plyr will handle a list
# or a vector as an "l" for us
# we want a simple array of scores back, so we use
# "l" + "a" + "ply" = "laply":
scores = laply(sentences, function(sentence, pos.words, neg.words) {
# clean up sentences with R's regex-driven global substitute, gsub():
sentence = gsub('[[:punct:]]', '', sentence)
sentence = gsub('[[:cntrl:]]', '', sentence)
sentence = gsub('\\d+', '', sentence)
# and convert to lower case:
sentence = tolower(sentence)
# split into words. str_split is in the stringr package
word.list = str_split(sentence, '\\s+')
# sometimes a list() is one level of hierarchy too much
words = unlist(word.list)
# compare our words to the dictionaries of positive & negative terms
pos.matches = match(words, pos.words)
neg.matches = match(words, neg.words)
# match() returns the position of the matched term or NA
# we just want a TRUE/FALSE:
pos.matches = !is.na(pos.matches)
neg.matches = !is.na(neg.matches)
# and conveniently enough, TRUE/FALSE will be treated as 1/0 by sum():
score = sum(pos.matches) - sum(neg.matches)
return(score)
}, pos.words, neg.words, .progress=.progress )
scores.df = data.frame(score=scores, text=sentences)
return(scores.df)
}
sentiment.scores= score.sentiment(customer_reviews, pos.words, neg.words, .progress='none')
score <- sentiment.scores$score
score
View(sentiment.scores)
View(positive_reviews)
library(DT)
library(tidytext)
library(dplyr)
library(stringr)
library(sentimentr)
library(ggplot2)
library(RColorBrewer)
library(readr)
library(SnowballC)
library(tm)
library(wordcloud)
library(reticulate)
library(crfsuite)
positive_reviews <- read.delim(file.choose(), header = FALSE)
str(positive_reviews)
attach(positive_reviews)
positive_reviews <- as.character(positive_reviews)
positive_reviews <- Corpus(VectorSource(V1))
print(positive_reviews)
positive_reviews <- tm_map(positive_reviews, tolower)
inspect(positive_reviews[1:6])
positive_reviews <- tm_map(positive_reviews,removePunctuation)
inspect(positive_reviews[1:6])
positive_reviews <- tm_map(positive_reviews,removeNumbers)
inspect(positive_reviews[1:6])
positive_reviews <-tm_map(positive_reviews,stripWhitespace)
inspect(positive_reviews[1:6])
cleanset_positive <- tm_map(positive_reviews,removeWords, stopwords('english'))
inspect(cleanset_positive[1:6])
tdm_positive <- TermDocumentMatrix(cleanset_positive)
tdm_positive
tdm_positive <- as.matrix(tdm_positive)
tdm_positive[1:10,1:20]
w <- rowSums(tdm_positive)  # provides the no of times a particular word has been used.
w <- subset(w, w>=10) # Pull words that were used more than 25 times.
barplot(w, las = 2, col = rainbow(50))
w <- rowSums(tdm_positive)  # provides the no of times a particular word has been used.
w <- subset(w, w>=10) # Pull words that were used more than 25 times.
barplot(w, las = 2, col = rainbow(50))
library(wordcloud2)
w <- data.frame(names(w),w)
colnames(w) <- c('word','freq')
wordcloud2(w,size = 0.5, shape = 'triangle', rotateRatio = 0.5, minSize = 2)
# Word Cloud :
w <- sort(rowSums(tdm_negative), decreasing = TRUE) # Sort words in decreasing order.
set.seed(123)
wordcloud(words = names(w), freq = w,
max.words = 250,random.order = F,
min.freq =  3,
colors = brewer.pal(8, 'Dark2'),
scale = c(5,0.3),
rot.per = 0.6)
# Word Cloud :
w <- sort(rowSums(tdm_negative), decreasing = TRUE) # Sort words in decreasing order.
set.seed(123)
negative_Reviews <- read.delim(file.choose())
str(negative_Reviews)
attach(negative_Reviews)
negative_Reviews <- as.character(x)
negative_courpus <- Corpus(VectorSource(x))
print(negative_Reviews)
negative_courpus <- tm_map(negative_courpus, tolower)
inspect(negative_courpus[1:6])
negative_courpus <- tm_map(negative_courpus,removePunctuation)
inspect(negative_courpus[1:6])
negative_courpus <- tm_map(negative_courpus,removeNumbers)
inspect(negative_courpus[1:6])
negative_courpus <-tm_map(negative_courpus,stripWhitespace)
inspect(negative_courpus[1:6])
cleanset_negative <- tm_map(negative_courpus,removeWords, stopwords('english'))
inspect(cleanset_negative[1:6])
tdm_negative <- TermDocumentMatrix(cleanset_negative)
tdm_negative
tdm_negative <- as.matrix(tdm_negative)
tdm[1:10,1:20]
w <- rowSums(tdm_negative)  # provides the no of times a particular word has been used.
w <- subset(w, w>=10) # Pull words that were used more than 25 times.
barplot(w, las = 2, col = rainbow(50))
# Word Cloud :
w <- sort(rowSums(tdm_negative), decreasing = TRUE) # Sort words in decreasing order.
set.seed(123)
wordcloud(words = names(w), freq = w,
max.words = 250,random.order = F,
min.freq =  3,
colors = brewer.pal(8, 'Dark2'),
scale = c(5,0.3),
rot.per = 0.6)
library(wordcloud2)
w <- data.frame(names(w),w)
colnames(w) <- c('word','freq')
wordcloud2(w,size = 0.5, shape = 'triangle', rotateRatio = 0.5, minSize = 2)
# Word Cloud :
w <- sort(rowSums(tdm_negative), decreasing = TRUE) # Sort words in decreasing order.
set.seed(123)
wordcloud(words = names(w), freq = w,
max.words = 250,random.order = F,
min.freq =  3,
colors = brewer.pal(8, 'Dark2'),
scale = c(5,0.3),
rot.per = 0.6)
library(SentimentAnalysis)
sentiment <- analyzeSentiment("Yeah, this was a great soccer game of the German team!")
convertToBinaryResponse(sentiment)$SentimentGI
# Create a vector of strings
documents <- c("Warm, friendly, courteous & helpful nature of the person. Listening with intent to understand, help & resolve issue customer was facing
problem sorted.",
"INCREASE NUMBER OF OUTLETS WHERE CARD IS ENTERTAINED",
"THE PERSON TALKED TO ME VERY RESPECTFULLY AND TO THE POINT. HE WAS VERY RESPONSIVE OF MY QUERRIES.",
"Better response from customer service is excepted.",
"Courteous and polite consultant",
"He was helpful and did tell me the best way to use my points.")
sentiment <- analyzeSentiment(documents)
library(syuzhet)
sentence <- "Warm, friendly, courteous & helpful nature of the person. Listening with intent to understand, help & resolve issue customer was facing
problem sorted."
library(syuzhet)
sentence <- "Warm, friendly, courteous & helpful nature of the person. Listening with intent to understand, help & resolve issue customer was facing
problem sorted."
sentence2 <- "INCREASE NUMBER OF OUTLETS WHERE CARD IS ENTERTAINED"
sentence3 <- "THE PERSON TALKED TO ME VERY RESPECTFULLY AND TO THE POINT. HE WAS VERY RESPONSIVE OF MY QUERRIES."
sentence4 <- "Better response from customer service is excepted."
sentence5 <- "He was helpful and did tell me the best way to use my points."
new_sentence <- as.character(strsplit(sentence, " "))
get_nrc_sentiment(new_sentence)
new_sentence <- as.character(strsplit(sentence2, " "))
get_nrc_sentiment(new_sentence)
new_sentence <- as.character(strsplit(sentence3, " "))
get_nrc_sentiment(new_sentence)
new_sentence <- as.character(strsplit(sentence4, " "))
get_nrc_sentiment(new_sentence)
new_sentence <- as.character(strsplit(sentence5, " "))
get_nrc_sentiment(new_sentence)
customer_reviews <- read.delim(file.choose())
View(customer_reviews)
str(customer_reviews)
attach(customer_reviews)
str(customer_reviews)
customer_reviews <- as.character(customer_reviews)
customer_courpus <- Corpus(VectorSource(comments))
print(customer_courpus)
customer_courpus <- tm_map(customer_courpus, tolower)
inspect(customer_courpus[1:6])
customer_courpus <- tm_map(customer_courpus,removePunctuation)
inspect(customer_courpus[1:6])
customer_courpus <- tm_map(customer_courpus,removeNumbers)
inspect(customer_courpus[1:6])
customer_courpus <-tm_map(customer_courpus,stripWhitespace)
inspect(customer_courpus[1:6])
cleanset <- tm_map(customer_courpus,removeWords, stopwords('english'))
inspect(cleanset[1:6])
tdm <- TermDocumentMatrix(cleanset)
tdm
#carryout sentiment mining using the get_nrc_sentiment()function #log the findings under a variable result
result <- get_nrc_sentiment(as.character(cleanset))
#change result from a list to a data frame and transpose it
result1<-data.frame(t(result))
#rowSums computes column sums across rows for each level of a #grouping variable.
new_result <- data.frame(rowSums(result1))
#name rows and columns of the dataframe
names(new_result)[1] <- "count"
new_result <- cbind("sentiment" = rownames(new_result), new_result)
rownames(new_result) <- NULL
#plot the first 5 rows,the distinct emotions
qplot(sentiment, data=new_result[1:5,], weight=count, geom="bar",fill=sentiment)+ggtitle("TedTalk Sentiments")
#plot the last 2 rows ,positive and negative
qplot(sentiment, data=new_result[9:10,], weight=count, geom="bar",fill=sentiment)+ggtitle("TedTalk Sentiments")
#plot the first 5 rows,the distinct emotions
qplot(sentiment, data=new_result[1:5,], weight=count, geom="bar",fill=sentiment)+ggtitle("TedTalk Sentiments")
#plot the first 5 rows,the distinct emotions
qplot(sentiment, data=new_result[1:5,], weight=count, geom="bar",fill=sentiment)+ggtitle("TedTalk Sentiments")
#name rows and columns of the dataframe
names(new_result)[1] <- "count"
new_result <- cbind("sentiment" = rownames(new_result), new_result)
rownames(new_result) <- NULL
#plot the first 5 rows,the distinct emotions
qplot(sentiment, data=new_result[1:5,], weight=count, geom="bar",fill=sentiment)+ggtitle("TedTalk Sentiments")
customer_courpus <- tm_map(customer_courpus,removePunctuation)
library(SentimentAnalysis)
